{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PINGPONG - NLP Submission Notebook","provenance":[],"authorship_tag":"ABX9TyM7EqfkmXfdFR+0iCHCA/ZG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **PINGPONG**\n","## NLP Code Submission\n","\n","BrainHack 2022 TIL\n","Main reference is https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5 .\n","\n","While the scores for NLP were not great (0.28 on final), we would really like to know which were the parts we had done wrongly / should have implemented. The work flow that we tried was \n","\n","\n","*   Augment dataset in 3 ways (Adding whitenoise, shifting, stretching) resulting in 4x the original data size (3500 * 4 = 1400)\n","*   Convert augmented and base audio into MFCC and save them out\n","*   Tried both CNN and a pretrained ResNet18, adjusted some weights and ran them.\n","*   Ran training and prediction.\n","\n","We would really appreciate if there was a way to get specific feedback on this project! Thank you for organizing the hackathon.\n","\n","\n","\n","\n","\n","\n"],"metadata":{"id":"478UWvMPyfa6"}},{"cell_type":"code","source":["#Imports\n","import librosa\n","import librosa.display\n","import matplotlib.pyplot as plt\n","import pylab\n","import torch\n","\n","import numpy as np\n","import re \n","import os\n","\n","import sklearn\n","import random\n","import skimage.io\n","\n","from torch.utils.data import DataLoader, Dataset, random_split\n","import torchaudio\n","from PIL import Image\n","import torchvision.transforms as transforms\n","import pandas as pd\n","\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import random_split\n","\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.nn import init\n","\n","from csv import writer\n","from google.colab import files"],"metadata":{"id":"-dM0vf3Iy2HJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Converting training images w class labels into **MFCC**"],"metadata":{"id":"nS6a33SE0Ca6"}},{"cell_type":"code","source":["#Run this code if needed to convert training images into MFCC \n","\n","#Audio Class w Data Augmentation functions\n","class AudioUtil:\n","\n","  def __init__(self):\n","    \"\"\"\n","    Utility class to help convert training audio clips into MFCCs.\n","    \"\"\"\n","    self.hop_length = 512 # number of samples per time-step in spectrogram\n","    self.n_mels = 312 # number of bins in spectrogram. Height of image\n","    self.time_steps = 100 # number of time-steps. Width of image\n","    pass\n","\n","  def open(self,inputfilepath,filepath,filename):\n","    \"\"\"\n","    Opens up an individual wav clip from the inputfilepath, performs 3 data augmentations (add whitenoise, stretch, shift) separately \n","    before converting to an MFCC image and exporting it to relevant folders.\n","    \"\"\"\n","    #Loading data and sampling rate with librosa with cut off time of 10seconds, do not think any clips exceed 10 seconds in duration.\n","    data, sampling_rate = librosa.core.load(inputfilepath, duration = 10)\n","    suffix = ['', '_whitenoise', '_shifted', '_stretched']\n","    #Changing file directory to save output\n","    os.chdir(filepath)\n","    #WhiteNoise Data\n","    data_w_wn = self.addWhiteNoise(data)\n","    #Shifted Data\n","    data_w_shift = self.shiftSound(data)\n","    #Stretched Data\n","    data_w_stretched = self.stretchAudio(data)\n","\n","    #Creating MFCCs for all of them, naming was a bit off so the final images had .png.png but worked fine.\n","    name = filename + suffix[0] +'.png'\n","    self.save(data,name,sampling_rate)    \n","\n","    name = filename + suffix[1] +'.png'\n","    self.save(data_w_wn,name,sampling_rate)  \n","\n","    name = filename + suffix[2] +'.png'\n","    self.save(data_w_shift,name,sampling_rate)  \n","\n","    name = filename + suffix[3] +'.png'\n","    self.save(data_w_stretched,name,sampling_rate)  \n","\n","  def save(self,data,name,sampling_rate):\n","    \"\"\"\n","    Helper function to save out MFCCS with appropriate filenames\n","    \"\"\"\n","    data = librosa.feature.mfcc(data, sr = sampling_rate)\n","    data = sklearn.preprocessing.scale(data, axis=1)\n","    librosa.display.specshow(data);\n","    pylab.axis('off') # no axis\n","    pylab.axes([0., 0., 1., 1.], frameon=False, xticks=[], yticks=[]) # Remove the white edge\n","    pylab.savefig(name, bbox_inches=None, pad_inches=None, transparent=True)\n","    pylab.close()\n","\n","  def scale_minmax(self,X, min=0.0, max=1.0):\n","    \"\"\"\n","    Normalizes the data.\n","    \"\"\"\n","    X_std = (X - X.min()) / (X.max() - X.min())\n","    X_scaled = X_std * (max - min) + min\n","    return X_scaled\n","\n","  # Data Augmentation Techniques below\n","  def addWhiteNoise(self,data):\n","    \"\"\"\n","    Adds in a small amount of white noise to the training data set to generalize better.\n","    \"\"\"\n","    wn = np.random.randn(len(data)) \n","    data_wn = data +0.005* wn\n","    return data_wn\n","\n","  def shiftSound(self,data):\n","    \"\"\"\n","    Shifts sound slightly\n","    \"\"\"\n","    data_roll = np.roll(data,1600)\n","    return data_roll\n","\n","  def stretchAudio(self,data):\n","    \"\"\"\n","    Stretches out audio\n","    \"\"\"\n","    input_length = 10000\n","    data = librosa.effects.time_stretch(data, 1)\n","    if len(data) > input_length:\n","      data = data[:input_length]\n","    else:\n","      data = np.pad(data, (0,max(0,input_length - len(data))), \"constant\")\n","    return data\n","\n","  def rechannel(self, data, new_channel):\n","    \"\"\"\n","    Converts mono to stereo, but was unable to get this to work for the input data hence unused.\n","    \"\"\"\n","    sig = data\n","    if (sig.shape[0] == new_channel):\n","      return sig\n","    if (new_channel == 1):\n","      # Convert from stereo to mono by selecting only the first channel\n","      resig = sig[:1, :]\n","    else:\n","      # Convert from mono to stereo by duplicating the first channel\n","      #resig = torch.cat([sig, sig])\n","      resig = np.array([sig, sig])\n","    return resig"],"metadata":{"id":"vCycxOUI0JSk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Converting training audio files into MFCC images**"],"metadata":{"id":"orVMTL4X4BbS"}},{"cell_type":"code","source":["#Run this code if you are converting training images into MFCC images\n","#Change path_main to be where the training images are, path_save to be where you want the output to be.\n","#Under the path_save, the code expects to have 5 folders (angry, sad, fear, neutral, happy) to save it in.\n","\n","#Create class\n","parseAudio = AudioUtil()\n","#Copy path to wav files\n","path_main='/content/drive/MyDrive/Qualifiers/NLP Training Dataset/ASR Training Dataset'\n","folders_main=os.listdir(path_main)\n","counter=0\n","for folders in folders_main :\n","    path_in='/content/drive/MyDrive/Qualifiers/NLP Training Dataset/ASR Training Dataset/{0}'.format(folders)\n","    files_sub=os.listdir(path_in)\n","    emotion= folders\n","    cacheDir=\"/content/pretrained_models/{0}\".format(emotion)\n","    os.makedirs(cacheDir, exist_ok = True)\n","    for file in files_sub :\n","      print(counter)      \n","      filename = (emotion + \"_\" + file.split('.')[0] + \".png\")\n","      path_save='/content/processedTrainingMFCCwAugmentation/{0}/'.format(emotion)\n","      path_load='{0}/{1}'.format(path_in,file)\n","      parseAudio.open(path_load,path_save, filename)\n","      counter += 1  \n","\n","#Creating zip archive to download it from Google Colab\n","shutil.make_archive('/content/processedMels', 'zip', '/content/processedMels')"],"metadata":{"id":"0p6WurkJ4GoU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Custom Data Loader for Training Data w Classes"],"metadata":{"id":"vzdSgGXH4iC5"}},{"cell_type":"code","source":["#Run this code if you are planning to retrain the model\n","\n","#Custom Data Loader for audio w classes\n","class SoundDS(Dataset):\n","  def __init__(self,filepath):\n","    self.filepath = filepath\n","  # ----------------------------\n","  # Number of items in dataset\n","  # ----------------------------\n","  def __len__(self):\n","    return len(os.listdir(self.filepath))\n","  # ----------------------------\n","  # Get i'th item in dataset\n","  # ----------------------------\n","  def __getitem__(self, idx): \n","    # Absolute file path of the audio file - concatenate the audio directory with\n","    # the relative path\n","    os.chdir(self.filepath) #Changing path to where all the spectograms are opened\n","    toOpen = os.listdir()[idx] #Getting the filename to open\n","    image = Image.open(self.filepath + '/' + toOpen)\n","    # Transforming image to tensors\n","    transform = transforms.Compose([\n","            transforms.PILToTensor()\n","        ])\n","    img_tensor = transform(image)\n","    #Getting and encoding classID from name\n","    classIDKey = {'angry':0, 'sad':1, 'happy':2 , 'neutral':3, 'fear':4}\n","    if \"_\" in toOpen:\n","      class_id = classIDKey[toOpen.split('_')[0]]\n","    return img_tensor,class_id"],"metadata":{"id":"zLRWrj1G4kc5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Custom Data Loader for Interim / Final Data w/o Classes"],"metadata":{"id":"QWmr6UkA806i"}},{"cell_type":"code","source":["#Run this code if you are planning to carry out predictions on test data without class labels.\n","\n","#Custom Data Loader without classes\n","#Does not return classID\n","class PredictSoundDS(Dataset):\n","  def __init__(self,filepath):\n","    self.filepath = filepath\n","  # ----------------------------\n","  # Number of items in dataset\n","  # ----------------------------\n","  def __len__(self):\n","    return len(os.listdir(self.filepath))\n","  # ----------------------------\n","  # Get i'th item in dataset\n","  # ----------------------------\n","  def __getitem__(self, idx): \n","    # Absolute file path of the audio file - concatenate the audio directory with\n","    # the relative path \n","    os.chdir(self.filepath) #Changing path to where all the spectograms are opened\n","    toOpen = os.listdir()[idx] #Getting the filename to open\n","    image = Image.open(self.filepath + '/' + toOpen)\n","    transform = transforms.Compose([\n","            transforms.PILToTensor()\n","        ])\n","    img_tensor = transform(image)\n","    return img_tensor,toOpen"],"metadata":{"id":"48bB78Ea85EB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Train-Test Splits\n","Initially, before working on the interim data we used a random train-test split and a stratified train-test split on the training data to get a train and validation set to get a handle of how our model was doing. \n","\n","We ended up finalizing training on the full training data before using that model for final results."],"metadata":{"id":"nemdZ9rA4zWa"}},{"cell_type":"code","source":["#Run either of the following three codes depending on what type of train-test split you are looking for.\n","#Change data_path to where the training / test images are. In the case of trying to load in the augmented data sets, we loaded them one folder at a time\n","#Eg, ran the whole training with base images, then with whitenoise, etc etc\n","#First 2 (Stratified + Random) were mostly used for training data\n","#Last (Full) was used to load the test images\n","\n","\n","# Stratified Test Split\n","data_path = \"/content/drive/MyDrive/Processed Augmented MFCC/Processed Augmented MFCC\" #File path of spectogram images\n","myds = SoundDS(data_path)\n","\n","#Splitting\n","num_items = len(myds)\n","train_indicies, val_indicies = train_test_split(list(range(num_items)), test_size = 0.2, stratify = [myds.__getitem__(x)[1] for x in range(num_items)] )\n","\n","train_dataset = torch.utils.data.Subset(myds, train_indicies)\n","val_dataset = torch.utils.data.Subset(myds, val_indicies)\n","\n","train_dl = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=False)\n","val_dl = torch.utils.data.DataLoader(val_dataset, batch_size=16, shuffle=False)"],"metadata":{"id":"t7MrTGW343tx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Train Test Split (random)\n","data_path = \"/content/drive/MyDrive/Processed Augmented MFCC/Processed Augmented MFCC\" #File path of spectogram images\n","myds = SoundDS(data_path)\n","# Random split of 80:20 between training and validation\n","num_items = len(myds)\n","num_train = round(num_items *0.8)\n","num_val = num_items - num_train\n","train_ds, val_ds = random_split(myds, [num_train, num_val])\n","# Create training and validation data loaders\n","train_dl = torch.utils.data.DataLoader(train_ds, batch_size=16, shuffle=True)\n","val_dl = torch.utils.data.DataLoader(val_ds, batch_size=16, shuffle=False)"],"metadata":{"id":"AcyBK4e95UeA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# No split, entire dataset is prepared by dataloader.\n","data_path = \"/content/drive/MyDrive/Processed Augmented MFCC/Processed Augmented MFCC/\" #File path of spectogram images\n","myds = SoundDS(data_path)\n","fulltrain_dl = torch.utils.data.DataLoader(myds, batch_size=35, shuffle=False)"],"metadata":{"id":"Sa32Zdy35agc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **Models**\n","\n","We tried out a custom CNN and using a pretrained ResNet18 for this. While both results were bad, we ended up using ResNet18 for the final submission as that was the one that was already solved."],"metadata":{"id":"KJeOGW5a5l3I"}},{"cell_type":"markdown","source":["## ResNet18 (Pretrained)\n","Similarly, for the pretrained ResNet18 the main changes were just to the input and output layers as well as some tweaks to the code to allow it to run on the dataset loaded. \n","\n","We used a CrossEntopyLoss and Stochastic Gradient Descent as criterion and optimizer respectively.\n","\n","The ResNet18 was the better performing one in training hence we saved and loaded the weights. "],"metadata":{"id":"hbvc54Iy6KSw"}},{"cell_type":"code","source":["#Run this to create the pretrained resnet18 and load our weights\n","\n","#ResNet\n","import torchvision.models as models\n","import torch.optim as optim\n","from  torch.cuda.amp import autocast\n","import torch.nn as nn\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","net = models.resnet18(pretrained=True)\n","net = net.to(device)\n","net.conv1 = nn.Conv2d(4, 64, (7, 7), (2, 2), (3, 3), bias=False).to(device)\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.SGD(net.parameters(), lr = 0.0001, momentum = 0.9)\n","def accuracy(out, labels):\n","    _,pred = torch.max(out, dim=1)\n","    return torch.sum(pred==labels).item()\n","num_ftrs = net.fc.in_features\n","net.fc = nn.Linear(num_ftrs, 5)\n","net.fc = net.fc.to(device)\n","next(net.parameters()).device\n","net.load_state_dict(torch.load('/content/myResNetModelv4.pt'))"],"metadata":{"id":"l2voaD_R6Uhx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ResNet18 (Training Code)"],"metadata":{"id":"-cbfdoid6x2x"}},{"cell_type":"code","source":["#Run this to carry out training.\n","\n","n_epochs = 5\n","print_every = 10\n","valid_loss_min = np.Inf\n","val_loss = []\n","val_acc = []\n","train_loss = []\n","train_acc = []\n","total_step = len(fulltrain_dl) #Change this base on which traintest split was loaded (fulltrain_d1 or train_dl)\n","for epoch in range(1, n_epochs+1):\n","    running_loss = 0.0\n","    correct = 0\n","    total=0\n","    print(f'Epoch {epoch}\\n')\n","    for batch_idx, (data_, target_) in enumerate(fulltrain_dl):#Change this base on which traintest split was loaded (fulltrain_d1 or train_dl)\n","        data_, target_ = data_.to(device), target_.to(device)\n","        optimizer.zero_grad()\n","        with autocast(): #Changed this\n","          outputs = net.forward(data_.type(torch.FloatTensor).to(device))\n","        loss = criterion(outputs, target_)\n","        loss.backward()\n","        optimizer.step()\n","        running_loss += loss.item()\n","        _,pred = torch.max(outputs, dim=1)\n","        correct += torch.sum(pred==target_).item()\n","        total += target_.size(0)\n","        if (batch_idx) % 20 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch, n_epochs, batch_idx, total_step, loss.item()))\n","    train_acc.append(100 * correct / total)\n","    train_loss.append(running_loss/total_step)\n","    print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n","    batch_loss = 0\n","    total_t=0\n","    correct_t=0\n","    with torch.no_grad(): #Evaluation block was commented out when doing training on the entire dataset as there was no test data\n","        net.eval()\n","        for data_t, target_t in (val_dl): \n","            data_t, target_t = data_t.to(device), target_t.to(device)\n","            with autocast(): #Changed this\n","              outputs_t = net.forward(data_t.type(torch.FloatTensor).to(device))\n","            loss_t = criterion(outputs_t, target_t)\n","            batch_loss += loss_t.item()\n","            _,pred_t = torch.max(outputs_t, dim=1)\n","            correct_t += torch.sum(pred_t==target_t).item()\n","            total_t += target_t.size(0)\n","        val_acc.append(100 * correct_t/total_t)\n","        val_loss.append(batch_loss/len(val_dl))\n","        network_learned = batch_loss < valid_loss_min\n","        print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')\n","        if network_learned:\n","            valid_loss_min = batch_loss\n","            torch.save(net.state_dict(), 'resnet.pt')\n","            print('Improvement-Detected, save-model')\n","    net.train()"],"metadata":{"id":"3s7Mu3DM60up"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ResNet18 (Prediction Code)"],"metadata":{"id":"5P1HHOBe7Q34"}},{"cell_type":"code","source":["#Run this to prep for prediction\n","\n","# ----------------------------\n","# Prediction\n","# ----------------------------\n","def predict (model, dataset):\n","  predicted_classes = []\n","  filename_list = []\n","  # Disable gradient updates\n","  with torch.no_grad():\n","    for data in dataset:\n","      # Get the input features and target labels, and put them on the GPU\n","      # inputs, filename= data[0].to(device), data[1].to(device)\n","      inputs = data[0].to(device)\n","      # Normalize the inputs\n","      inputs_m, inputs_s = (inputs*1.0).mean(), (inputs*1.0).std()\n","      inputs = (inputs - inputs_m) / inputs_s\n","      # Get predictions\n","      outputs = model(inputs)\n","      # Get the predicted class with the highest score\n","      _, prediction = torch.max(outputs,1)\n","      predicted_classes += (prediction.tolist())\n","      filename_list += data[1]\n","  return predicted_classes, filename_list"],"metadata":{"id":"15En2yN27aa4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Run this to use ResNet to predict on the test data and download a CSV\n","\n","#After exporting, file was sorted in excel by column A.\n","path = \"/content/STUFF MFCC\" #File path of test images for prediction\n","interimDS = PredictSoundDS(path)\n","print(f\"This is {len(interimDS)}\")\n","interim_data = torch.utils.data.DataLoader(interimDS, batch_size=35, shuffle=False)\n","prediction, filename = predict(net, interim_data)\n","#Converting predictions to export\n","# class_id = {'angry':0, 'sad':1, 'happy':2 , 'neutral':3, 'fear':4}\n","emotion_key ={0:'angry', 1:'sad', 2:'happy', 3:'neutral', 4:'fear'}\n","with open('/content/resnet_submission.csv', 'w') as fi:\n","  writer_object = writer(fi)\n","  for i,clip in enumerate(filename):\n","    clip = clip.split('.')[0] + '.wav'\n","    line = clip, emotion_key[prediction[i]]\n","    writer_object.writerow(line)\n","  fi.close\n","files.download('/content/resnet_submission.csv')"],"metadata":{"id":"sNOS503o7e2R"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNN\n","For the CNN, the main thing we changed was just the first input layer to accept 4 channels (should corrospond to RGBA of a PNG image) and the output layer (Linear Classifier) to support 5 classes instead of 10."],"metadata":{"id":"VEhm4AAn6uqK"}},{"cell_type":"code","source":["#Implementing CNN\n","#courtesy of https://towardsdatascience.com/audio-deep-learning-made-simple-sound-classification-step-by-step-cebc936bbe5\n","# ----------------------------\n","# Audio Classification Model\n","# ----------------------------\n","class AudioClassifier (nn.Module):\n","    # ----------------------------\n","    # Build the model architecture\n","    # ----------------------------\n","    def __init__(self):\n","        super().__init__()\n","        conv_layers = []\n","        # First Convolution Block with Relu and Batch Norm. Use Kaiming Initialization\n","        self.conv1 = nn.Conv2d(4, 8, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2))\n","        self.relu1 = nn.ReLU()\n","        self.bn1 = nn.BatchNorm2d(8)\n","        init.kaiming_normal_(self.conv1.weight, a=0.1)\n","        self.conv1.bias.data.zero_()\n","        conv_layers += [self.conv1, self.relu1, self.bn1]\n","        # Second Convolution Block\n","        self.conv2 = nn.Conv2d(8, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu2 = nn.ReLU()\n","        self.bn2 = nn.BatchNorm2d(16)\n","        init.kaiming_normal_(self.conv2.weight, a=0.1)\n","        self.conv2.bias.data.zero_()\n","        conv_layers += [self.conv2, self.relu2, self.bn2]\n","        # Second Convolution Block\n","        self.conv3 = nn.Conv2d(16, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu3 = nn.ReLU()\n","        self.bn3 = nn.BatchNorm2d(32)\n","        init.kaiming_normal_(self.conv3.weight, a=0.1)\n","        self.conv3.bias.data.zero_()\n","        conv_layers += [self.conv3, self.relu3, self.bn3]\n","        # Second Convolution Block\n","        self.conv4 = nn.Conv2d(32, 64, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n","        self.relu4 = nn.ReLU()\n","        self.bn4 = nn.BatchNorm2d(64)\n","        init.kaiming_normal_(self.conv4.weight, a=0.1)\n","        self.conv4.bias.data.zero_()\n","        conv_layers += [self.conv4, self.relu4, self.bn4]\n","        # Linear Classifier\n","        self.ap = nn.AdaptiveAvgPool2d(output_size=1)\n","        self.lin = nn.Linear(in_features=64, out_features=5)\n","        # Wrap the Convolutional Blocks\n","        self.conv = nn.Sequential(*conv_layers)\n","    # ----------------------------\n","    # Forward pass computations\n","    # ----------------------------\n","    def forward(self, x):\n","        # Run the convolutional blocks\n","        x = self.conv(x)\n","        # Adaptive pool and flatten for input to linear layer\n","        x = self.ap(x)\n","        x = x.view(x.shape[0], -1)\n","        # Linear layer\n","        x = self.lin(x)\n","        # Final output\n","        return x\n","# Create the model and put it on the GPU if available\n","myModel = AudioClassifier()\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","myModel = myModel.to(device)\n","# Check that it is on Cuda\n","next(myModel.parameters()).device"],"metadata":{"id":"JKHeqTX05wGj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNN (Training Code)"],"metadata":{"id":"mo-6TypW78eA"}},{"cell_type":"code","source":["# ----------------------------\n","# Training Loop\n","# ----------------------------\n","def training(model, train_dl, num_epochs):\n","  # Loss Function, Optimizer and Scheduler\n","  criterion = nn.CrossEntropyLoss()\n","  optimizer = torch.optim.Adam(model.parameters(),lr=0.001)\n","  scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=0.001,\n","                                                steps_per_epoch=int(len(train_dl)),\n","                                                epochs=num_epochs,\n","                                                anneal_strategy='linear')\n","  # Repeat for each epoch\n","  for epoch in range(num_epochs):\n","    running_loss = 0.0\n","    correct_prediction = 0\n","    total_prediction = 0\n","    # Repeat for each batch in the training set\n","    for i, data in enumerate(train_dl):\n","        # print(i)\n","    #print(len(train_dl))\n","    # for data in train_dl:     \n","        # Get the input features and target labels, and put them on the GPU\n","        inputs, labels = data[0].to(device), data[1].to(device)\n","        # # Normalize the inputs\n","        inputs_m, inputs_s = (inputs*1.0).mean(), (inputs*1.0).std()\n","        inputs = (inputs - inputs_m) / inputs_s\n","        # Zero the parameter gradients\n","        optimizer.zero_grad()\n","        # forward + backward + optimize\n","        outputs = model(inputs)\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","        # Keep stats for Loss and Accuracy\n","        running_loss += loss.item()\n","        # Get the predicted class with the highest score\n","        _, prediction = torch.max(outputs,1)\n","        # Count of predictions that matched the target label\n","        correct_prediction += (prediction == labels).sum().item()\n","        total_prediction += prediction.shape[0]\n","        if i % 10 == 0:    # print every 10 mini-batches\n","           print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss / 10))    \n","    # Print stats at the end of the epoch\n","    num_batches = len(train_dl)\n","    avg_loss = running_loss / num_batches\n","    acc = correct_prediction/total_prediction\n","    print(f'Epoch: {epoch}, Loss: {avg_loss:.2f}, Accuracy: {acc:.2f}')\n","  print('Finished Training')\n","num_epochs=12  # Just for demo, adjust this higher.\n","training(myModel, fulltrain_dl, num_epochs) #Change second parameter based on which train test split was used, either fulltrain_dl or train_dl\n","torch.save(myModel.state_dict(), '/content/myAudioModel.pt')\n","files.download('/content/myAudioModel.pt')\n"],"metadata":{"id":"H0YfkQal7-Q5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CNN (Prediction Code)"],"metadata":{"id":"NKNBwAVa8P3X"}},{"cell_type":"code","source":["# ----------------------------\n","# Inference\n","# ----------------------------\n","def inference (model, val_dl):\n","  correct_prediction = 0\n","  total_prediction = 0\n","  predicted_classes = []\n","  # Disable gradient updates\n","  with torch.no_grad():\n","    for data in val_dl:\n","      # Get the input features and target labels, and put them on the GPU\n","      inputs, labels = data[0].to(device), data[1].to(device)\n","      # Normalize the inputs\n","      inputs_m, inputs_s = (inputs*1.0).mean(), (inputs*1.0).std()\n","      inputs = (inputs - inputs_m) / inputs_s\n","      # Get predictions\n","      outputs = model(inputs)\n","      # Get the predicted class with the highest score\n","      _, prediction = torch.max(outputs,1)\n","      # Count of predictions that matched the target label\n","      correct_prediction += (prediction == labels).sum().item()\n","      predicted_classes.append(prediction)\n","      total_prediction += 1\n","  return predicted_classes\n","  acc = correct_prediction/total_prediction\n","  print(f'Accuracy: {acc:.2f}')\n","# Run inference on trained model with the validation set\n","inference(myModel, val_dl) #Only applicable if used either random split or stratify"],"metadata":{"id":"sfHPFNMF8Phj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Predicting on the main file\n","#Excel file was sorted inside excel after\n","path = \"/content/interimProccessedMels (MFCC)\" #File path of spectogram images for prediction\n","interimDS = PredictSoundDS(path)\n","print(f\"This is {len(interimDS)}\")\n","interim_data = torch.utils.data.DataLoader(interimDS, batch_size=35, shuffle=False)\n","prediction, filename = prediction(myModel, interim_data)\n","emotion_key ={0:'angry', 1:'sad', 2:'happy', 3:'neutral', 4:'fear'}\n","with open('/content/submission.csv', 'w') as fi:\n","  writer_object = writer(fi)\n","  for i,clip in enumerate(filename):\n","    clip = clip.split('.')[0] + '.wav'\n","    line = clip, emotion_key[prediction[i]]\n","    writer_object.writerow(line)\n","  fi.close\n","from google.colab import files\n","files.download('/content/submission.csv')"],"metadata":{"id":"7S7-29w38c5G"},"execution_count":null,"outputs":[]}]}